{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning course: Homework 1\n",
    "### The Frozen Lake problem\t\t\t\t\n",
    "It is winter, and you are hiking in the mountains. You end up in an area with a big frozen lake. On the other end of the lake, there is food, and you are hungry and all your resources are done. There is no way to make it back to the city. You need to cross the lake and take the food in order to survive. \n",
    "\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you’ll fall into the freezing water. Moreover, there are piles of snow (walls) which you can not step over. However, the ice is slippery, so you won’t always move in the direction you intend. There is 0.8 probability of moving one step in the direction you want vs. 0.2 probability of sliding and going two steps ahead. The actions are UP, DOWN, WEST, EAST.\n",
    "\t\t\t\t\n",
    "In order to take the optimal actions you need to solve this problem as a discounted MDP with γ= 0.9. We provide you the grid.txt for generating the frozen lake.\n",
    "\n",
    "**Requirements: Python 3.8 **\n",
    "\n",
    "**Use the MushroomRL library to solve the problem**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mushroom_rl in c:\\program files\\python38\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: joblib in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (1.1.0)\n",
      "Requirement already satisfied: scipy in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (4.64.0)\n",
      "Requirement already satisfied: Cython in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (0.29.30)\n",
      "Requirement already satisfied: matplotlib in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (3.5.2)\n",
      "Requirement already satisfied: pytest in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (7.1.2)\n",
      "Requirement already satisfied: opencv-python in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (4.6.0.66)\n",
      "Requirement already satisfied: numpy-ml in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (0.1.2)\n",
      "Requirement already satisfied: torch in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (1.12.0)\n",
      "Requirement already satisfied: pygame in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\program files\\python38\\lib\\site-packages (from mushroom_rl) (1.23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\program files\\python38\\lib\\site-packages (from scikit-learn->mushroom_rl) (3.1.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\program files\\python38\\lib\\site-packages (from tqdm->mushroom_rl) (0.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (1.4.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (9.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\program files\\python38\\lib\\site-packages (from matplotlib->mushroom_rl) (21.3)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (1.11.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0; sys_platform == \"win32\" in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\program files\\python38\\lib\\site-packages (from pytest->mushroom_rl) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\program files\\python38\\lib\\site-packages (from torch->mushroom_rl) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python38\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mushroom_rl) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 22.1.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install mushroom_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "plt.ion()\n",
    "# mushroom\n",
    "from mushroom_rl.environments.generators.grid_world import parse_grid, compute_reward, compute_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the MDP of the provided grid using the FiniteMDP class in Mushroom, and modify it accordingly to acquire the probability transition matrix, the reward, the initial state probability distribution, etc. write a text interface to display the current state (visualize agent in the text grid). Use the given grid.txt to create the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frozen_lake_grid(grid, prob, pos_rew, neg_rew, gamma=.9, horizon=100):\n",
    "    \"\"\"\n",
    "    This Frozen Lake Grid World generator requires a .txt file to specify the\n",
    "    shape of the grid world and the cells. There are five types of cells: 'S' is\n",
    "    the starting position where the agent is; 'G' is the goal state; '.' is a\n",
    "    normal cell; '*' is a hole, when the agent steps on a hole, it receives a\n",
    "    negative reward and the episode ends; '#' is a wall, when the agent is\n",
    "    supposed to step on a wall, it actually remains in its current state. The\n",
    "    initial states distribution is uniform among all the initial states\n",
    "    provided.\n",
    "\n",
    "    The grid is expected to be rectangular.\n",
    "\n",
    "    Args:\n",
    "        grid (str): the path of the file containing the grid structure;\n",
    "        prob (float): probability of success of an action;\n",
    "        pos_rew (float): reward obtained in goal states;\n",
    "        neg_rew (float): reward obtained in \"hole\" states;\n",
    "        gamma (float, .9): discount factor;\n",
    "        horizon (int, 100): the horizon.\n",
    "\n",
    "    Returns:\n",
    "        A FrozenLakeFiniteMDP object built with the provided parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # [YOUR CODE]\n",
    "    grid_map, cell_list = parse_grid(grid)\n",
    "    p = compute_probabilities_frozenlake(grid_map, cell_list, prob)\n",
    "    rew = compute_reward(grid_map, cell_list, pos_rew, neg_rew)\n",
    "    mu = compute_mu(grid_map, cell_list)\n",
    "\n",
    "    return FrozenLakeFiniteMDP(p, rew, mu, gamma, horizon), grid_map, cell_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probabilities_frozenlake(grid_map, cell_list, prob):\n",
    "    \"\"\"\n",
    "    Compute the transition probability matrix.\n",
    "\n",
    "    Args:\n",
    "        grid_map (list): list containing the grid structure;\n",
    "        cell_list (list): list of non-wall cells;\n",
    "        prob (float): probability of success of an action.\n",
    "\n",
    "    Returns:\n",
    "        The transition probability matrix;\n",
    "\n",
    "    \"\"\"\n",
    "    # HINT: Look at the implementation of compute_probabilities in mushroom_rl.environments.generators.grid_world\n",
    "    \n",
    "    # [YOUR CODE]\n",
    "    g = np.array(grid_map)\n",
    "    c = np.array(cell_list)\n",
    "    n_states = len(cell_list)\n",
    "    p = np.zeros((n_states, 4, n_states))\n",
    "    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        state = c[i]\n",
    "        #print(state)\n",
    "        if g[tuple(state)] in ['.', 'S']:\n",
    "            for a in range(len(directions)):\n",
    "                new_state = state + directions[a]\n",
    "                j = np.where((c == new_state).all(axis=1))[0]\n",
    "                slip_state = new_state + directions[a]\n",
    "                j2 = np.where((c == slip_state).all(axis=1))[0]\n",
    "                \n",
    "                if j.size > 0:\n",
    "                    assert j.size == 1\n",
    "                    \n",
    "                    if (j2.size > 0):\n",
    "                        assert j2.size == 1\n",
    "\n",
    "                        p[i, a, j] = prob\n",
    "                        p[i, a, j2] = 1. - prob\n",
    "\n",
    "                    else:\n",
    "                        p[i, a, j] = 1.\n",
    "                else:\n",
    "                    p[i, a, i] = 1.\n",
    "                    # prob nicht success not, sondern double?\n",
    "                    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.core import Environment, MDPInfo\n",
    "from mushroom_rl.utils import spaces\n",
    "from mushroom_rl.environments.finite_mdp import FiniteMDP\n",
    "from mushroom_rl.utils.viewer import Viewer\n",
    "\n",
    "class FrozenLakeFiniteMDP(FiniteMDP):\n",
    "    \"\"\"\n",
    "    Frozen Lake Finite Markov Decision Process.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_grid(state, width):\n",
    "        return np.array([state[0] // width, state[0] % width])\n",
    "    \n",
    "    def render(self, grid_map, state):\n",
    "        list_state = self.convert_to_grid(state, 10)\n",
    "        i = 0\n",
    "        for row in grid_map:\n",
    "            if list_state[0] == i:\n",
    "                row[list_state[1]] += 'A'\n",
    "            print(row)\n",
    "            i += 1\n",
    "    pass\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[0 1]\n",
      "[0 3]\n",
      "[0 4]\n",
      "[0 5]\n",
      "[0 7]\n",
      "[0 8]\n",
      "[1 0]\n",
      "[1 1]\n",
      "[1 2]\n",
      "[1 3]\n",
      "[1 4]\n",
      "[1 5]\n",
      "[1 7]\n",
      "[1 8]\n",
      "[1 9]\n",
      "[2 1]\n",
      "[2 3]\n",
      "[2 4]\n",
      "[2 5]\n",
      "[2 6]\n",
      "[2 7]\n",
      "[2 8]\n",
      "[2 9]\n",
      "[3 0]\n",
      "[3 1]\n",
      "[3 2]\n",
      "[3 4]\n",
      "[3 5]\n",
      "[3 6]\n",
      "[3 7]\n",
      "[3 8]\n",
      "[3 9]\n",
      "[4 0]\n",
      "[4 1]\n",
      "[4 2]\n",
      "[4 3]\n",
      "[4 4]\n",
      "[4 5]\n",
      "[4 7]\n",
      "[4 8]\n",
      "[4 9]\n",
      "[5 0]\n",
      "[5 1]\n",
      "[5 2]\n",
      "[5 3]\n",
      "[5 4]\n",
      "[5 6]\n",
      "[5 7]\n",
      "[5 8]\n",
      "[5 9]\n",
      "[6 2]\n",
      "[6 3]\n",
      "[6 5]\n",
      "[6 8]\n",
      "[6 9]\n",
      "[7 0]\n",
      "[7 1]\n",
      "[7 2]\n",
      "[7 3]\n",
      "[7 4]\n",
      "[7 5]\n",
      "[7 8]\n",
      "[8 0]\n",
      "[8 1]\n",
      "[8 2]\n",
      "[8 3]\n",
      "[8 4]\n",
      "[8 5]\n",
      "[8 6]\n",
      "[8 7]\n",
      "[8 8]\n",
      "[8 9]\n",
      "[9 1]\n",
      "[9 2]\n",
      "[9 3]\n",
      "[9 4]\n",
      "[9 5]\n",
      "[9 6]\n",
      "[9 8]\n",
      "[9 9]\n",
      "['SA', '.', '#', '.', '*', '.', '#', '*', '.', '#']\n",
      "['.', '.', '.', '.', '.', '.', '#', '.', '.', '.']\n",
      "['#', '.', '#', '*', '.', '.', '.', '.', '.', '.']\n",
      "['.', '.', '*', '#', '.', '.', '.', '.', '.', '*']\n",
      "['*', '.', '.', '.', '*', '*', '#', '.', '.', '.']\n",
      "['.', '.', '.', '.', '*', '#', '*', '*', '.', '.']\n",
      "['#', '#', '.', '.', '#', '*', '#', '#', '.', '*']\n",
      "['*', '.', '.', '.', '.', '*', '#', '#', '.', '#']\n",
      "['.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n",
      "['#', '.', '.', '*', '.', '.', '*', '#', '.', 'G']\n"
     ]
    }
   ],
   "source": [
    "grid = \"grid.txt\"\n",
    "prob = 0.8\n",
    "pos_rew = 1.0\n",
    "neg_rew = -1.0\n",
    "gamma = 0.99\n",
    "horizon = 100\n",
    "mdp, grid_map, cell_list = generate_frozen_lake_grid(grid, prob = prob, pos_rew = pos_rew, neg_rew= neg_rew, gamma= gamma, horizon = horizon)\n",
    "state = mdp.reset()\n",
    "mdp.render(grid_map, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic programming on The Frozen Lake\n",
    "1. Solve the frozen lake problem with Policy Iteration (PI) and Value Iteration (VI). Report the value function and the optimal policy for PI and VI. Compare the results. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mushroom_rl.solvers.dynamic_programming import policy_iteration, value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83172241 0.86052679 0.70406351 0.         0.89756981 0.\n",
      " 0.9261367  0.86052679 0.87297579 0.88165145 0.88944173 0.89702037\n",
      " 0.90452113 0.92227653 0.93392157 0.92227653 0.88165145 0.\n",
      " 0.90760428 0.91520385 0.9230537  0.93002902 0.94177184 0.93002902\n",
      " 0.70406351 0.88944173 0.         0.90010038 0.90749947 0.91603872\n",
      " 0.91919353 0.94968823 0.         0.         0.89702037 0.90760428\n",
      " 0.90010038 0.         0.         0.94572949 0.95767052 0.94572949\n",
      " 0.89756981 0.90452113 0.91520385 0.90749947 0.         0.\n",
      " 0.         0.96572311 0.7648527  0.9230537  0.91603872 0.\n",
      " 0.97382736 0.         0.         0.92227653 0.93002902 0.91919353\n",
      " 0.94572949 0.         0.98208    0.9261367  0.93392157 0.94177184\n",
      " 0.94968823 0.95767052 0.96572311 0.97382736 0.98208    0.99\n",
      " 1.         0.92227653 0.93002902 0.         0.94572949 0.7648527\n",
      " 0.         1.         0.        ]\n",
      "(array([0.85882513, 0.86750013, 0.70453027, 0.        , 0.89759635,\n",
      "       0.        , 0.9261367 , 0.86750013, 0.87479201, 0.88214576,\n",
      "       0.88955842, 0.89704792, 0.90452763, 0.92227666, 0.93392157,\n",
      "       0.92227666, 0.88214576, 0.        , 0.90760653, 0.91520438,\n",
      "       0.92305383, 0.93002905, 0.94177184, 0.93002905, 0.70453027,\n",
      "       0.88955842, 0.        , 0.90011892, 0.90750385, 0.91603975,\n",
      "       0.91919378, 0.94968823, 0.        , 0.        , 0.89704792,\n",
      "       0.90760653, 0.90011892, 0.        , 0.        , 0.94572949,\n",
      "       0.95767052, 0.94572949, 0.89759635, 0.90452763, 0.91520438,\n",
      "       0.90750385, 0.        , 0.        , 0.        , 0.96572311,\n",
      "       0.7648527 , 0.92305383, 0.91603975, 0.        , 0.97382736,\n",
      "       0.        , 0.        , 0.92227666, 0.93002905, 0.91919378,\n",
      "       0.94572949, 0.        , 0.98208   , 0.9261367 , 0.93392157,\n",
      "       0.94177184, 0.94968823, 0.95767052, 0.96572311, 0.97382736,\n",
      "       0.98208   , 0.99      , 1.        , 0.92227666, 0.93002905,\n",
      "       0.        , 0.94572949, 0.7648527 , 0.        , 1.        ,\n",
      "       0.        ]), array([1, 1, 1, 0, 1, 0, 1, 3, 1, 3, 3, 1, 1, 3, 1, 2, 1, 0, 3, 3, 3, 3,\n",
      "       1, 2, 3, 1, 0, 3, 3, 3, 0, 1, 0, 0, 3, 1, 1, 0, 0, 3, 1, 2, 3, 3,\n",
      "       1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 0, 1, 1, 2, 1, 0, 1, 3, 3, 3,\n",
      "       3, 3, 3, 3, 3, 1, 1, 0, 0, 0, 0, 0, 0, 3, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(value_iteration(mdp.p, mdp.r, gamma, 0.1))\n",
    "print(policy_iteration(mdp.p, mdp.r, gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Value_iteration and policy_iteration give a similar optimal value matrix (in form of an array)\n",
    " as output, but policy_iteration additionally outputs a policy (which does not seem right,\n",
    " as it wants to go through the first wall, go up, if it is right left of the goal, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Modify the Policy Iteration method in Mushroom so that it can accept a policy initialization and stopping criterion (in terms of number of iterations). Initialize with a policy that always goes EAST. Compare the obtained Value Function and Policy compared to your result in q.1. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frozen_lake_policy_iteration(prob, reward, gamma, initial_policy = None, n_iterations = -1):\n",
    "    \"\"\"\n",
    "    Policy iteration algorithm to solve a dynamic programming problem.\n",
    "\n",
    "    Args:\n",
    "        prob (np.ndarray): transition probability matrix;\n",
    "        reward (np.ndarray): reward matrix;\n",
    "        gamma (float): discount factor.\n",
    "        initial_policy (int  or np.ndarray): value to intialize the policy.\n",
    "                        if np.ndarray is provided, use it as the initial policy.\n",
    "        n_iterations (int): number of iterations to run the algorithm. \n",
    "                    if -1 is provided, the stopping criterion is the convergence.\n",
    "\n",
    "    Returns:\n",
    "        The optimal value of each state and the optimal policy.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = prob.shape[0]\n",
    "    n_actions = prob.shape[1]\n",
    "    \n",
    "    # policy \n",
    "    if initial_policy is None:\n",
    "        policy = np.zeros(n_states, dtype=int)\n",
    "    elif type(initial_policy) is int:\n",
    "        policy = np.zeros(n_states, dtype=int)\n",
    "        for i in range(len(policy)):\n",
    "            policy[i] = initial_policy\n",
    "    else:\n",
    "        policy = initial_policy\n",
    "    \n",
    "    value = np.zeros(n_states)\n",
    "\n",
    "    if type(n_iterations) is int and n_iterations > 0:\n",
    "        changed = True\n",
    "        for i in range(n_iterations):\n",
    "            p_pi = np.zeros((n_states, n_states))\n",
    "            r_pi = np.zeros(n_states)\n",
    "            i = np.eye(n_states)\n",
    "\n",
    "            for state in range(n_states):\n",
    "                action = policy[state]\n",
    "                p_pi_s = prob[state, action, :]\n",
    "                r_pi_s = reward[state, action, :]\n",
    "\n",
    "                p_pi[state, :] = p_pi_s.T\n",
    "                r_pi[state] = p_pi_s.T.dot(r_pi_s)\n",
    "\n",
    "            value = np.linalg.inv(i - gamma * p_pi).dot(r_pi)\n",
    "\n",
    "            changed = False\n",
    "\n",
    "            for state in range(n_states):\n",
    "                vmax = value[state]\n",
    "                for action in range(n_actions):\n",
    "                    if action != policy[state]:\n",
    "                        p_sa = prob[state, action]\n",
    "                        r_sa = reward[state, action]\n",
    "                        va = p_sa.T.dot(r_sa + gamma * value)\n",
    "                        if va > vmax and not np.isclose(va, vmax):\n",
    "                            policy[state] = action\n",
    "                            vmax = va\n",
    "                            changed = True\n",
    "            \n",
    "            if changed == False:\n",
    "                break\n",
    "                \n",
    "    else:\n",
    "        changed = True\n",
    "        while changed:\n",
    "            p_pi = np.zeros((n_states, n_states))\n",
    "            r_pi = np.zeros(n_states)\n",
    "            i = np.eye(n_states)\n",
    "\n",
    "            for state in range(n_states):\n",
    "                action = policy[state]\n",
    "                p_pi_s = prob[state, action, :]\n",
    "                r_pi_s = reward[state, action, :]\n",
    "\n",
    "                p_pi[state, :] = p_pi_s.T\n",
    "                r_pi[state] = p_pi_s.T.dot(r_pi_s)\n",
    "\n",
    "            value = np.linalg.inv(i - gamma * p_pi).dot(r_pi)\n",
    "\n",
    "            changed = False\n",
    "\n",
    "            for state in range(n_states):\n",
    "                vmax = value[state]\n",
    "                for action in range(n_actions):\n",
    "                    if action != policy[state]:\n",
    "                        p_sa = prob[state, action]\n",
    "                        r_sa = reward[state, action]\n",
    "                        va = p_sa.T.dot(r_sa + gamma * value)\n",
    "                        if va > vmax and not np.isclose(va, vmax):\n",
    "                            policy[state] = action\n",
    "                            vmax = va\n",
    "                            changed = True\n",
    "\n",
    "    return value, policy\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.85882513, 0.86750013, 0.70453027, 0.        , 0.89759635,\n",
       "        0.        , 0.9261367 , 0.86750013, 0.87479201, 0.88214576,\n",
       "        0.88955842, 0.89704792, 0.90452763, 0.92227666, 0.93392157,\n",
       "        0.92227666, 0.88214576, 0.        , 0.90760653, 0.91520438,\n",
       "        0.92305383, 0.93002905, 0.94177184, 0.93002905, 0.70453027,\n",
       "        0.88955842, 0.        , 0.90011892, 0.90750385, 0.91603975,\n",
       "        0.91919378, 0.94968823, 0.        , 0.        , 0.89704792,\n",
       "        0.90760653, 0.90011892, 0.        , 0.        , 0.94572949,\n",
       "        0.95767052, 0.94572949, 0.89759635, 0.90452763, 0.91520438,\n",
       "        0.90750385, 0.        , 0.        , 0.        , 0.96572311,\n",
       "        0.7648527 , 0.92305383, 0.91603975, 0.        , 0.97382736,\n",
       "        0.        , 0.        , 0.92227666, 0.93002905, 0.91919378,\n",
       "        0.94572949, 0.        , 0.98208   , 0.9261367 , 0.93392157,\n",
       "        0.94177184, 0.94968823, 0.95767052, 0.96572311, 0.97382736,\n",
       "        0.98208   , 0.99      , 1.        , 0.92227666, 0.93002905,\n",
       "        0.        , 0.94572949, 0.7648527 , 0.        , 1.        ,\n",
       "        0.        ]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3,\n",
       "        1, 2, 3, 1, 1, 3, 3, 3, 0, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 3,\n",
       "        1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 1, 0, 0, 1, 0, 0, 1, 3, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen_lake_policy_iteration(mdp.p, mdp.r, gamma, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have a lot more 1s, where we have had 0s (e.g. 4, last two zeros), and some more 3s, suggesting a more efficient policy\n",
    " (more movement right and down)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Take the policy of Iteration #5 from q.2 of PI. Do Monte Carlo Policy Evaluation (first visit). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random \n",
    "\n",
    "class monte_carlo():\n",
    "\n",
    "    def run(self, state):\n",
    "        print(self.visited[0])\n",
    "        if self.g[tuple(state)] in ['G']:\n",
    "            print('goal')\n",
    "            mc_value = +1\n",
    "        elif self.g[tuple(state)] in ['*']:\n",
    "            print('hole')\n",
    "            mc_value = -1\n",
    "        else: \n",
    "            direction = self.pol[cell_list.index(state)]\n",
    "            #print(direction)\n",
    "            new_state = [state[0] + self.directions[direction][0],state[1] + self.directions[direction][1]]\n",
    "            #print(new_state)\n",
    "            new_state_number = np.where((self.c == new_state).all(axis=1))[0]\n",
    "            # one walk exists, otherwise 0\n",
    "            if (self.c == new_state_number).any():\n",
    "                #print(new_state)\n",
    "                # check absorbing\n",
    "                if self.g[tuple(new_state)] in ['G']:\n",
    "                    mc_value = self.gamma * self.run(new_state)\n",
    "                elif self.g[tuple(new_state)] in ['*']:\n",
    "                    mc_value = self.gamma * self.run(new_state)\n",
    "                else:\n",
    "                    # check if slip exists\n",
    "                    new_state_slip = [new_state[0] + self.directions[direction][0],new_state[1] + self.directions[direction][1]]\n",
    "                    #print(new_state_slip)\n",
    "                    new_state_slip_number = np.where((self.c == new_state_slip).all(axis=1))[0]\n",
    "                    #print(new_state_slip_number)\n",
    "                    if new_state_slip_number.size > 0:\n",
    "                        if random() < 0.8:\n",
    "                #            print('random1')\n",
    "                            if new_state in self.visited[0]:\n",
    "               #                 print('we visited 1?')\n",
    "              #                  print(new_state)\n",
    "                                mc_value = 0\n",
    "                            else: \n",
    "             #                   print('new 1!')\n",
    "            #                    print(new_state)\n",
    "                                self.visited[0][new_state_number] = new_state_number\n",
    "                                mc_value = self.gamma * self.run(new_state)\n",
    "                        else:\n",
    "                 #           print('random2')\n",
    "                  #          print(new_state_slip)\n",
    "                            if new_state_slip in self.visited[0]:\n",
    "                                mc_value = 0\n",
    "                            else: \n",
    "                                self.visited[0][new_state_number] = new_state_number\n",
    "                                mc_value = self.gamma * self.run(new_state_slip)\n",
    "\n",
    "\n",
    "                    # if no slip, only new_state\n",
    "                    elif new_state_number in self.visited[0]:\n",
    "                        mc_value = 0\n",
    "                    else:\n",
    "                        self.visited[0][new_state_number] = new_state_number\n",
    "                        mc_value = self.gamma * self.run(new_state)\n",
    "        \n",
    "            else: \n",
    "                mc_value = 0\n",
    "        if mc_value != 0:        \n",
    "            state_number = np.where((self.c == state).all(axis=1))[0]        \n",
    "            self.visited[1][state_number] = mc_value\n",
    "            print('unequal 0!')\n",
    "        return mc_value\n",
    "    \n",
    "    def __init__(self, cell_list, grid_map, pol, val, gamma, iterations):\n",
    "        self.g = np.array(grid_map)\n",
    "        self.c = np.array(cell_list)\n",
    "        self.pol = pol\n",
    "        self.val = val\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        self.rewards = np.zeros((iterations, len(cell_list)))\n",
    "        start_state = [0,0]\n",
    "        mc_values = 0\n",
    "        self.directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            self.visited = np.zeros((2, len(cell_list)))\n",
    "            \n",
    "            mc_values += self.run(start_state)\n",
    "            self.rewards[i] = self.visited[1]\n",
    "            print('iteration done')\n",
    "        \n",
    "        print(self.rewards)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<__main__.monte_carlo object at 0x000001A9A970ABB0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knarf\\AppData\\Local\\Temp\\ipykernel_20524\\2064155896.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if new_state in self.visited[0]:\n",
      "C:\\Users\\knarf\\AppData\\Local\\Temp\\ipykernel_20524\\2064155896.py:48: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if new_state_slip in self.visited[0]:\n"
     ]
    }
   ],
   "source": [
    "val, pol = frozen_lake_policy_iteration(mdp.p, mdp.r, gamma, 1, 15)\n",
    "print(monte_carlo(cell_list, grid_map, pol, val, gamma, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implement the exact policy evaluation using the matrix-based formulation of the Bellman equation and compare it with the approximated one computed in q.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 7. 8. 9. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "iteration done\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "<__main__.bellman_equ object at 0x000001A9AE8CC190>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\knarf\\AppData\\Local\\Temp\\ipykernel_20524\\2430767276.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if new_state in self.visited[0]:\n",
      "C:\\Users\\knarf\\AppData\\Local\\Temp\\ipykernel_20524\\2430767276.py:47: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if new_state_slip in self.visited[0]:\n"
     ]
    }
   ],
   "source": [
    "from random import random \n",
    "\n",
    "class bellman_equ():\n",
    "\n",
    "    def run(self, state):\n",
    "        print(self.visited[0])\n",
    "        if self.g[tuple(state)] in ['G']:\n",
    "            print('goal')\n",
    "            mc_value = +1\n",
    "        elif self.g[tuple(state)] in ['*']:\n",
    "            print('hole')\n",
    "            mc_value = -1\n",
    "        else: \n",
    "            direction = self.pol[cell_list.index(state)]\n",
    "            #print(direction)\n",
    "            new_state = [state[0] + self.directions[direction][0],state[1] + self.directions[direction][1]]\n",
    "            #print(new_state)\n",
    "            new_state_number = np.where((self.c == new_state).all(axis=1))[0]\n",
    "            # one walk exists, otherwise 0\n",
    "            if (self.c == new_state_number).any():\n",
    "                #print(new_state)\n",
    "                # check absorbing\n",
    "                if self.g[tuple(new_state)] in ['G']:\n",
    "                    mc_value = self.gamma * self.run(new_state)\n",
    "                elif self.g[tuple(new_state)] in ['*']:\n",
    "                    mc_value = self.gamma * self.run(new_state)\n",
    "                else:\n",
    "                    # check if slip exists\n",
    "                    new_state_slip = [new_state[0] + self.directions[direction][0],new_state[1] + self.directions[direction][1]]\n",
    "                    #print(new_state_slip)\n",
    "                    new_state_slip_number = np.where((self.c == new_state_slip).all(axis=1))[0]\n",
    "                    #print(new_state_slip_number)\n",
    "                    if new_state_slip_number.size > 0:\n",
    "                        # both states exist\n",
    "                #            print('random1')\n",
    "                            if new_state in self.visited[0]:\n",
    "               #                 print('we visited 1?')\n",
    "              #                  print(new_state)\n",
    "                                mc_value = 0\n",
    "                            else: \n",
    "             #                   print('new 1!')\n",
    "            #                    print(new_state)\n",
    "                                self.visited[0][new_state_number] = new_state_number\n",
    "                                mc_value = 0.8 * self.gamma * self.run(new_state)\n",
    "                 #           print('random2')\n",
    "                  #          print(new_state_slip)\n",
    "                            if new_state_slip in self.visited[0]:\n",
    "                                slip_mc_value = 0\n",
    "                            else: \n",
    "                                self.visited[0][new_state_number] = new_state_number\n",
    "                                slip_mc_value = 0.2 * self.gamma * self.run(new_state_slip)\n",
    "                            mc_value = mc_value + slip_mc_value\n",
    "\n",
    "                    # if no slip, only new_state\n",
    "                    elif new_state_number in self.visited[0]:\n",
    "                        mc_value = 0\n",
    "                    else:\n",
    "                        self.visited[0][new_state_number] = new_state_number\n",
    "                        mc_value = self.gamma * self.run(new_state)\n",
    "        \n",
    "            else: \n",
    "                mc_value = 0\n",
    "        if mc_value != 0:        \n",
    "            state_number = np.where((self.c == state).all(axis=1))[0]        \n",
    "            self.visited[1][state_number] = mc_value\n",
    "            print('unequal 0!')\n",
    "        return mc_value\n",
    "    \n",
    "    def __init__(self, cell_list, grid_map, pol, val, gamma, iterations):\n",
    "        self.g = np.array(grid_map)\n",
    "        self.c = np.array(cell_list)\n",
    "        self.pol = pol\n",
    "        self.val = val\n",
    "        self.gamma = gamma\n",
    "        self.iterations = iterations\n",
    "        self.rewards = np.zeros((iterations, len(cell_list)))\n",
    "        start_state = [0,0]\n",
    "        mc_values = 0\n",
    "        self.directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            self.visited = np.zeros((2, len(cell_list)))\n",
    "            \n",
    "            mc_values += self.run(start_state)\n",
    "            self.rewards[i] = self.visited[1]\n",
    "            print('iteration done')\n",
    "        \n",
    "        print(self.rewards)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(bellman_equ(cell_list, grid_map, pol, val, gamma, 5))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Well, they are the same, though they do not run properly. If they ran properly, then with increasing iterations of MC, \n",
    " it should get closer and closer to the bellman matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free RL on The Frozen Lake\n",
    "1. Train the same MDP with Q-Learning, SARSA with $\\epsilon$-Greedy and Bolt\n",
    "zmann policies. Run 25 random seeds and plot the evaluation curves for all combinations of algorithms and policies, when you evaluate the policy every n steps. Plot the evaluation curves average returns J w.r.t. epochs. Explain the differences of the obtained policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mushroom\n",
    "from mushroom_rl.algorithms.value import SARSA, QLearning\n",
    "from mushroom_rl.core import Core, Logger\n",
    "from mushroom_rl.environments import *\n",
    "from mushroom_rl.policy import Boltzmann, EpsGreedy\n",
    "from mushroom_rl.utils.parameters import Parameter, ExponentialParameter, LinearParameter\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from mushroom_rl.utils.callbacks import CollectDataset, CollectMaxQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "def get_mean_and_confidence(data):\n",
    "    \"\"\"\n",
    "    Compute the mean and 95% confidence interval\n",
    "    Args:\n",
    "        data (np.ndarray): Array of experiment data of shape (n_runs, n_epochs).\n",
    "    Returns:\n",
    "        The mean of the dataset at each epoch along with the confidence interval.\n",
    "    \"\"\"\n",
    "    mean = np.mean(data, axis=0)\n",
    "    se = st.sem(data, axis=0)\n",
    "    n = len(data)\n",
    "    interval, _ = st.t.interval(0.95, n-1, scale=se)\n",
    "    return mean, interval\n",
    "\n",
    "\n",
    "def plot_mean_conf(data, ax, color='blue', line='-', facecolor=None, alpha=0.4, label=None):\n",
    "    \"\"\"\n",
    "    Method to plot mean and confidence interval for data on pyplot axes.\n",
    "    \"\"\"\n",
    "    facecolor = color if facecolor is None else facecolor\n",
    "\n",
    "    mean, conf = get_mean_and_confidence(np.array(data))\n",
    "    upper_bound = mean + conf\n",
    "    lower_bound = mean - conf\n",
    "\n",
    "    ax.plot(mean, color=color, linestyle=line, label=label)\n",
    "    ax.fill_between(np.arange(np.size(mean)), upper_bound, lower_bound, facecolor=facecolor, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_run = 25\n",
    "num_epochs = 200\n",
    "n_steps = 1000\n",
    "n_episodes_test = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': .01,\n",
    "    'eps': 1.,\n",
    "    'eps_test': 0.,\n",
    "    'beta': 100000.,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(pi, agent_class, params, num_epochs=100, n_steps=1000, n_episodes_test=5, seed=1):\n",
    "    # HINT: Use exponetial decay for EpsGreedy\n",
    "    # [YOUR CODE]\n",
    "    # pi = policy\n",
    "    # agent_class = agent\n",
    "    print('oh oh')\n",
    "    # Policy\n",
    "    #epsilon_test = params['eps_test']\n",
    "    epsilon = ExponentialParameter(value=params['eps'], exp=.5, size=mdp.info.observation_space.size)\n",
    "    if pi == 'EpsGreedy':\n",
    "        pi = EpsGreedy(epsilon=epsilon)\n",
    "    else:\n",
    "        pi = Boltzmann(epsilon=epsilon)\n",
    "    \n",
    "    # Agent\n",
    "    learning_rate = ExponentialParameter(value=params['learning_rate'], exp=0.9, size=mdp.info.size)\n",
    "    algorithm_params = dict(learning_rate=learning_rate)\n",
    "    if agent_class == 'QLearning':\n",
    "        agent = QLearning(mdp.info, pi, **algorithm_params)\n",
    "        print('Q Started!')\n",
    "    else:\n",
    "        agent = SARSA(mdp.info, pi, **algorithm_params)\n",
    "        print('sarsa go!')\n",
    "    \n",
    "    # Algorithm\n",
    "    start = 0\n",
    "    collect_max_Q = CollectMaxQ(agent.Q, start)\n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset, collect_max_Q]\n",
    "    core = Core(agent, mdp, callbacks)\n",
    "    \n",
    "    # Train\n",
    "    core.learn(n_steps=n_steps, n_episodes_per_fit=5)\n",
    "    \n",
    "    _, _, reward, _, _, _ = parse_dataset(collect_dataset.get())\n",
    "    max_Qs = collect_max_Q.get()\n",
    "    \n",
    "    return reward, max_Qs\n",
    "\n",
    "def experiment(policies, agents, params, num_run=25, num_epochs=100, n_steps=1000, n_episodes_test=5):\n",
    "    seeds = np.random.randint(0, 1e5, size=(num_run,))  # list of seeds\n",
    "    data = {}\n",
    "    # pipeline\n",
    "    for pi in policies:\n",
    "        for agent_class in agents:\n",
    "            Js = Parallel(n_jobs=-1)(delayed(run)(\n",
    "                pi, agent_class, params,\n",
    "                num_epochs=num_epochs,\n",
    "                n_steps=n_steps,\n",
    "                n_episodes_test=n_episodes_test,\n",
    "                seed=seeds[i]\n",
    "            ) for i in range(num_run))\n",
    "            data[pi.__name__ + '_' + agent_class.__name__] = np.array(Js)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\program files\\python38\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"c:\\program files\\python38\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\program files\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\program files\\python38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\program files\\python38\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\knarf\\AppData\\Local\\Temp\\ipykernel_20524\\444948885.py\", line 33, in run\n  File \"c:\\program files\\python38\\lib\\site-packages\\mushroom_rl\\core\\core.py\", line 75, in learn\n    self._run(n_steps, n_episodes, fit_condition, render, quiet)\n  File \"c:\\program files\\python38\\lib\\site-packages\\mushroom_rl\\core\\core.py\", line 125, in _run\n    return self._run_impl(move_condition, fit_condition, steps_progress_bar,\n  File \"c:\\program files\\python38\\lib\\site-packages\\mushroom_rl\\core\\core.py\", line 156, in _run_impl\n    self.agent.fit(dataset)\n  File \"c:\\program files\\python38\\lib\\site-packages\\mushroom_rl\\algorithms\\value\\td\\td.py\", line 32, in fit\n    assert len(dataset) == 1\nAssertionError\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [209]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m policies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpsGreedy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoltzmann\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQLearning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSARSA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [208]\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(policies, agents, params, num_run, num_epochs, n_steps, n_episodes_test)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pi \u001b[38;5;129;01min\u001b[39;00m policies:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent_class \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[1;32m---> 46\u001b[0m         Js \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_episodes_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_run\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m         data[pi\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m agent_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Js)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\program files\\python38\\lib\\concurrent\\futures\\_base.py:388\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get_result\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policies = ['EpsGreedy', 'Boltzmann']\n",
    "agents = ['QLearning', 'SARSA']\n",
    "data = experiment(policies, agents, params, num_run=num_run, num_epochs=num_epochs, n_steps=n_steps, n_episodes_test=n_episodes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "for i, v in enumerate(data.items()): \n",
    "    plot_mean_conf(v[1], ax, color=colors[i], label=v[0])\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"J\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS**: Visualize the Grid with a GUI (e.g. matplotlib or pygame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def changed_el(element):\n",
    "    if str(element).__contains__('G'):\n",
    "        i = 1\n",
    "    if str(element).__contains__('S'):\n",
    "        i = 0.8\n",
    "    if str(element).__contains__('*'):\n",
    "        i = 0.6\n",
    "    if str(element).__contains__('.'):\n",
    "        i = 0.4\n",
    "    if str(element).__contains__('A'):\n",
    "        i = 0.2\n",
    "    if str(element).__contains__('#'):\n",
    "        i = 0\n",
    "    return i\n",
    "\n",
    "image = [[changed_el(a) for a in row] for row in grid_map]\n",
    "\n",
    "plt.matshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(cell_list, pol, val, gamma, iterations):\n",
    "    state_count = np.zeros(len(cell_list))\n",
    "    state_return = np.zeros(len(cell_list))\n",
    "    average_return = np.zeros(len(cell_list))\n",
    "    current_states = cell_list.copy()\n",
    "    directions = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "    c = np.array(cell_list)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        unvisited_states = cell_list.copy()\n",
    "        for cell_number in range(len(cell_list)):\n",
    "            cell = current_states[cell_number]\n",
    "            if cell in ['G']:\n",
    "                if cell in unvisited_states:\n",
    "                    unvisited_states.remove(cell)\n",
    "                    state_count[cell_number] += 1\n",
    "                    state_return[cell_number] += 1 * pow(gamma, i)\n",
    "            elif cell in ['*']:\n",
    "                if cell in unvisited_states:\n",
    "                    unvisited_states.remove(cell)\n",
    "                    state_count[cell_number] += 1\n",
    "                    state_return[cell_number] += -1 * pow(gamma, i)\n",
    "            else:\n",
    "                direction = pol[cell_number]\n",
    "                new_state = [cell[0] + directions[direction][0],cell[1] + directions[direction][1]]\n",
    "                new_cell = np.where((c == new_state).all(axis=1))[0]\n",
    "                \n",
    "                if direction == 0:\n",
    "                    new_cell_number = cell_number -1 \n",
    "                if direction == 1:\n",
    "                    new_cell_number = cell_number + 1 \n",
    "                if direction == 2:\n",
    "                    new_cell_number = cell_number -10 \n",
    "                if direction == 3:\n",
    "                    new_cell_number = cell_number + 10\n",
    "                # checking absorbing states\n",
    "                if new_cell.size > 0:    \n",
    "                    if new_cell in ['G']:\n",
    "                        if (unvisited_states == new_cell).all(1).any():\n",
    "                            unvisited_states.remove(new_cell)\n",
    "                            state_count[new_cell_number] += 1\n",
    "                            state_return[cell_number] += 1  * pow(gamma, i)\n",
    "                    elif new_cell in ['*']:\n",
    "                        if (unvisited_states == new_cell).all(1).any():\n",
    "                            unvisited_states = np.setdiff1d(unvisited_states, new_cell)\n",
    "                            state_count[new_cell_number] += 1\n",
    "                            state_return[cell_number] += -1 * pow(gamma, i)\n",
    "                    else:\n",
    "                        # not sliding\n",
    "                        if random() < 0.8:\n",
    "                            if (unvisited_states == new_cell).any():\n",
    "                                unvisited_states = np.setdiff1d(unvisited_states, new_cell)\n",
    "                                state_count[new_cell_number] += 1\n",
    "                                state_return[cell_number] += average_return[new_cell_number] * pow(gamma, i)\n",
    "                        # sliding\n",
    "                        else: \n",
    "                            new_cell = cell + 2*directions[direction]\n",
    "                            if (type(unvisited_states) == list()) and (unvisited_states == new_cell).any():\n",
    "                                if direction == 0:\n",
    "                                    new_cell_number = new_cell_number -1 \n",
    "                                if direction == 1:\n",
    "                                    new_cell_number = new_cell_number + 1 \n",
    "                                if direction == 2:\n",
    "                                    new_cell_number = new_cell_number -10 \n",
    "                                if direction == 3:\n",
    "                                    new_cell_number = new_cell_number + 10\n",
    "                                    \n",
    "                                if new_cell in ['G']:\n",
    "                                    if (unvisited_states == new_cell).all(1).any():\n",
    "                                        unvisited_states = np.setdiff1d(unvisited_states, new_cell)\n",
    "                                        state_count[new_cell_number] += 1\n",
    "                                        state_return[new_cell_number] += 1  * pow(gamma, i)\n",
    "                                elif new_cell in ['*']:\n",
    "                                    if (unvisited_states == new_cell).all(1).any():\n",
    "                                        unvisited_states = np.setdiff1d(unvisited_states, new_cell)\n",
    "                                        state_count[new_cell_number] += 1\n",
    "                                        state_return[new_cell_number] += -1  * pow(gamma, i)\n",
    "                                else:\n",
    "                                    unvisited_states = np.setdiff1d(unvisited_states, new_cell)\n",
    "                                    state_count[new_cell_number] += 1\n",
    "                                    state_return[cell_number] += average_return[new_cell_number]  * pow(gamma, i)\n",
    "\n",
    "        for q in range(len(state_count)):\n",
    "            if state_count[q] > 0:\n",
    "                average_return[q] = state_return[q] / state_count[q]\n",
    "        print('for iteration ') \n",
    "        print(i) \n",
    "        print(average_return)\n",
    "        print(state_return) \n",
    "        print(state_count)\n",
    "    return average_return"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "701f933d40042698dd8d227e8dcff65879a145680838631c64a6cbfe05911989"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
